# Master's Thesis Code  

This repository contains the code for my master's thesis, which introduces a novel approach for evaluating scientific question answering. It leverages adversarial datasets to assess the performance of large language models (LLMs) as evaluators.  

## Overview  
- Implements a structured adversarial evaluation framework  
- Tests LLMs' ability to detect subtle and extreme quality degradations  
- Includes datasets and evaluation scripts  
